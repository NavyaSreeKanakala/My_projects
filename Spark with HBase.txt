

  $HBASE_PATH= '/usr/lib/hbase/lib classpath'
  
  spark-shell --driver-class-path '$HBASE_PATH'

  import org.apache.hadoop.hbase.HBaseConfiguration
 
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
 
import org.apache.hadoop.hbase.client.HBaseAdmin
 
import org.apache.hadoop.hbase.{HTableDescriptor,HColumnDescriptor}
 
import org.apache.hadoop.hbase.util.Bytes
 
import org.apache.hadoop.hbase.client.{Put,HTable}


val conf = HBaseConfiguration.create()
val tablename = "example_spark_hbase"
conf.set(TableInputFormat.INPUT_TABLE,tablename)
val admin = new HBaseAdmin(conf)


if(!admin.isTableAvailable(tablename)){
 
print("creating table:"+tablename+"\t")
 
val tableDescription = new HTableDescriptor(tablename)
 
tableDescription.addFamily(new HColumnDescriptor("cf".getBytes()));
 
admin.createTable(tableDescription);
 
} else {
 
print("table already exists")
 
}

admin.isTableAvailable(tablename)

admin.getTableNames()


val table = new HTable(conf,tablename);
 
for(x <- 1 to 10){
 
var p = new Put(new String("row" + x).getBytes());
 
p.add("colfamily1".getBytes(),"column1".getBytes(),new String("value" + x).getBytes());
 
table.put(p);
 
}
  
  val HbaseRDD = sc.newAPIHadoopRDD(conf,classOf[TableInputFormat],classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])
  
  val count = HbaseRDD.count()
  
  
  
  
  
  
  spark-submit   --driver-class-path '/usr/lib/hbase/lib classpath' --class HBaseReadWrite --master local